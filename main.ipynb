{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-20T15:53:21.847428500Z",
     "start_time": "2024-02-20T15:53:16.325356200Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# simple dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8750a3b4c79a6a8f"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your device is cuda\n",
      "datasets have been created\n",
      "dataloaders have been created\n",
      "there are 2 classes, and class names are ['myxo', 'non-myxo']\n",
      "Dataset sizes: {'train': 445, 'val': 188}\n",
      "Class 0: 191 instances\n",
      "Class 1: 385 instances\n"
     ]
    }
   ],
   "source": [
    "### preparing Data\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}')\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.255])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Grayscale(),\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Grayscale(),\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_path = 'D:\\Master Project\\model\\model-1\\data'\n",
    "datasets = {x: ImageFolder(root=os.path.join(data_path, x), transform=data_transforms[x]) for x in ['train', 'val']}\n",
    "print('datasets have been created')\n",
    "\n",
    "dataloaders = {x: DataLoader(dataset=datasets[x], batch_size=batch_size, num_workers=2, shuffle=False, drop_last=True)\n",
    "               for x in ['train', 'val']}\n",
    "print('dataloaders have been created')\n",
    "\n",
    "class_names = datasets['train'].classes\n",
    "print(f'there are {len(class_names)} classes, and class names are {class_names}')\n",
    "\n",
    "class_counts = {x: len(datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Dataset sizes: {class_counts}')\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for _, label in dataloaders[phase]:\n",
    "        class_counts.update(label.tolist())\n",
    "\n",
    "# show details\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count} instances')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:36:29.086966200Z",
     "start_time": "2024-02-19T17:36:19.788894100Z"
    }
   },
   "id": "cd0c7be8ff004fd5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Augmented dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "324c9e198ea6180f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your device is cuda\n",
      "Datasets have been created.\n",
      "Dataloaders have been created.\n",
      "There are 2 classes, and class names are ['myxo', 'non-myxo']\n",
      "Dataset sizes: {'train': 620, 'val': 264}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 436 instances\n",
      "Class 0: 428 instances\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class to handle oversampling\n",
    "class BalancedImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None, minority_class=0, augment_transforms=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.minority_class = minority_class\n",
    "        self.augment_transforms = augment_transforms\n",
    "\n",
    "        # Identify the minority and majority classes and their counts\n",
    "        class_counts = self._get_class_counts()\n",
    "        self.max_count = max(class_counts.values())\n",
    "        self.indices = self._oversample_indices(class_counts)\n",
    "\n",
    "    def _get_class_counts(self):\n",
    "        class_counts = {}\n",
    "        for _, label in self.samples:\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "        return class_counts\n",
    "\n",
    "    def _oversample_indices(self, class_counts):\n",
    "        # Oversample minority indices\n",
    "        minority_indices = [i for i, (_, label) in enumerate(self.samples) if label == self.minority_class]\n",
    "        required_samples = self.max_count - class_counts[self.minority_class]\n",
    "        oversampled_minority_indices = resample(minority_indices, replace=True, n_samples=required_samples, random_state=123)\n",
    "        return list(range(len(self.samples))) + oversampled_minority_indices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        actual_index = self.indices[index]\n",
    "        path, target = self.samples[actual_index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if target == self.minority_class and self.augment_transforms is not None:\n",
    "            sample = self.augment_transforms(sample)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}')\n",
    "\n",
    "# # Data normalization mean and std\n",
    "# mean = np.array([0.485, 0.456, 0.406])\n",
    "# std = np.array([0.229, 0.224, 0.255])\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Additional augmentations for the minority class\n",
    "minority_augment_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "])\n",
    "\n",
    "# Standard data transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(1040),\n",
    "        # transforms.Normalize(mean, std),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Resize(256),\n",
    "        transforms.CenterCrop(1040),\n",
    "        # transforms.Normalize(mean, std),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Path to your data\n",
    "data_path = 'D:\\\\Master Project\\\\model\\\\model-1\\\\data'\n",
    "\n",
    "# Creating datasets with BalancedImageFolder\n",
    "datasets = {\n",
    "    x: BalancedImageFolder(\n",
    "        root=os.path.join(data_path, x),\n",
    "        transform=data_transforms[x],\n",
    "        minority_class=0,  \n",
    "        augment_transforms=minority_augment_transforms if x == 'train' else None,\n",
    "    ) for x in ['train', 'val']\n",
    "}\n",
    "print('Datasets have been created.')\n",
    "\n",
    "# Creating dataloaders\n",
    "dataloaders = {\n",
    "    x: DataLoader(dataset=datasets[x], batch_size=batch_size, num_workers=0, shuffle=True if x == 'train' else False, drop_last=True)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "print('Dataloaders have been created.')\n",
    "\n",
    "# Get class names\n",
    "class_names = datasets['train'].classes\n",
    "print(f'There are {len(class_names)} classes, and class names are {class_names}')\n",
    "\n",
    "# Count class instances\n",
    "class_counts = {x: len(datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Dataset sizes: {class_counts}')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for images, labels in dataloaders[phase]:\n",
    "        # Ensure labels are moved to CPU and converted to list if necessary\n",
    "        labels_list = labels.cpu().numpy().tolist() if device == 'cuda' else labels.tolist()\n",
    "        class_counts.update(labels_list)\n",
    "\n",
    "# Show details\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count} instances')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T15:55:24.993672600Z",
     "start_time": "2024-02-20T15:54:57.442807100Z"
    }
   },
   "id": "98d608428fb4fb7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## simple cnn model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49d4ee4922331ff"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1       [-1, 32, 1040, 1040]             896\n",
      "         MaxPool2d-2         [-1, 32, 520, 520]               0\n",
      "            Conv2d-3         [-1, 64, 520, 520]          18,496\n",
      "         MaxPool2d-4         [-1, 64, 260, 260]               0\n",
      "            Conv2d-5        [-1, 128, 260, 260]          73,856\n",
      "         MaxPool2d-6        [-1, 128, 130, 130]               0\n",
      "            Conv2d-7        [-1, 256, 130, 130]         295,168\n",
      "         MaxPool2d-8          [-1, 256, 65, 65]               0\n",
      "            Conv2d-9          [-1, 512, 65, 65]       1,180,160\n",
      "        MaxPool2d-10          [-1, 512, 32, 32]               0\n",
      "           Linear-11                    [-1, 2]       1,048,578\n",
      "================================================================\n",
      "Total params: 2,617,154\n",
      "Trainable params: 2,617,154\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.38\n",
      "Forward/backward pass size (MB): 639.40\n",
      "Params size (MB): 9.98\n",
      "Estimated Total Size (MB): 661.76\n",
      "----------------------------------------------------------------\n",
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=524288, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(512 * 32 * 32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv4(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv5(x)))\n",
    "        x = x.view(-1, 512 * 32 * 32)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleCNN()\n",
    "input_size = (3, 1040, 1040)\n",
    "model = model.to(device)\n",
    "summary(model, input_size)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T15:57:11.415880900Z",
     "start_time": "2024-02-20T15:57:11.068177400Z"
    }
   },
   "id": "9ac1b3360c0f32b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pretrained model: efficientnet_v2_s"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17da805df71f9a41"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Dropout(p=0.2, inplace=True)\n  (1): Linear(in_features=1280, out_features=2, bias=True)\n)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "last_layer = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names), bias=True)\n",
    ")\n",
    "\n",
    "model.classifier = last_layer\n",
    "model.classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:44:43.337057700Z",
     "start_time": "2024-02-19T17:44:43.120780600Z"
    }
   },
   "id": "c095fc045ba2bddd"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "\n",
    "# train function \n",
    "def train_model(model, criterion, optimizer, dataloaders, datasets, epoch_num=25):\n",
    "    acc_list = EasyDict({'train': [], 'val': []})\n",
    "    loss_list = EasyDict({'train': [], 'val': []})\n",
    "\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            s0 = datetime.now()\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over batches and data for training and validation\n",
    "            for idx, batch in enumerate(dataloaders[phase], 0):\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Transfer data and labels to CUDA if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                    # Back Propagation and updating weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "            # Calculating Accuracy and Loss per phase\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_accuracy = running_corrects.double() / len(datasets[phase])\n",
    "\n",
    "            # Show epoch details\n",
    "            delta = datetime.now() - s0\n",
    "            print(f'{phase.capitalize()} Accuracy: {epoch_accuracy:.4f} | Loss: {epoch_loss:.4f} | time: {delta}')\n",
    "\n",
    "            # Copy the model weights if its better\n",
    "            if phase == 'val' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('Best model weights updated!')\n",
    "\n",
    "            # Save Loss and accuracy\n",
    "            acc_list[phase].append(epoch_accuracy)\n",
    "            loss_list[phase].append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    print(f'Best Accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "    # Loading best model weights \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_list, loss_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T16:01:43.478046300Z",
     "start_time": "2024-02-20T16:01:43.460045300Z"
    }
   },
   "id": "af8ddccf3dc2d4b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n"
     ]
    }
   ],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "model = model.to(device)\n",
    "# train model\n",
    "model, acc_lists, loss_lists = train_model(model, criterion, optimizer, dataloaders, datasets, epoch_num=10)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-20T16:01:43.789266200Z"
    }
   },
   "id": "c726d9e2b87f0f9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_lists.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T16:01:41.414746400Z"
    }
   },
   "id": "ae95ec69c97b6ef8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a for a in loss_lists.train], label='train')\n",
    "plt.plot([a for a in loss_lists.val if a < 1], label='val')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T16:01:41.415745500Z"
    }
   },
   "id": "91bd40ce7cca8561"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1761d17db595abc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    model.eval()\n",
    "    nrows, ncols = 4, 4\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                img = inputs.cpu().data[j]\n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                axes[i][j].axis('off')\n",
    "                axes[i][j].set_title(\n",
    "                    f'predictions: {class_names[predictions[j]]}, label: {class_names[labels[j]]}'\n",
    "                )\n",
    "                axes[i][j].imshow(img)\n",
    "                if j == ncols - 1:\n",
    "                    break\n",
    "            if i == nrows - 1:\n",
    "                break\n",
    "\n",
    "\n",
    "visualize_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-20T16:01:41.416746400Z"
    }
   },
   "id": "676185da7a3dfdb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "65a95b76bbff37f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
