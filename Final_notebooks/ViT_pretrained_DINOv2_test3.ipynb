{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1410a8d31948e1e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ViT Small Pretrained on DINOv2 with registers \n",
    "## Data: Myxococcaceae vs non-Myxococcaceae\n",
    "## Augmentation: TrivialAugmentWide to 60000 samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a7389c7dee4cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:51:28.176958500Z",
     "start_time": "2024-04-11T13:51:26.341600800Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms, autoaugment\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da703ffb580a457e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6895498d51d47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Loading Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb23dc65e3ffa848",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T13:51:28.283959Z",
     "start_time": "2024-04-11T13:51:28.176958500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your device is cuda\n",
      "datasets have been created\n",
      "dataloaders have been created\n",
      "there are 2 classes, and class names are ['Myxococcaceae', 'non-Myxococcaceae']\n",
      "Dataset sizes: {'train': 4558, 'val': 1139}\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}')\n",
    "\n",
    "# Defining data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop((224, 224))\n",
    "])\n",
    "\n",
    "data_path = 'D:\\Master Project\\model\\model-1\\myxo-vs-nonmyxo-V2-9p'\n",
    "dataset = ImageFolder(root=data_path, transform=data_transforms)\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "# Create indices for splitting\n",
    "targets = torch.tensor(dataset.targets)\n",
    "class_counts = targets.unique(return_counts=True)[1]\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Calculating indices for each class to maintain the ratio\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_idx, val_idx = [], []\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    class_indices = [i for i in range(len(targets)) if targets[i] == class_index]\n",
    "    split = int(len(class_indices) * validation_split)\n",
    "    class_indices = torch.tensor(class_indices)[torch.randperm(len(class_indices))].tolist()\n",
    "    \n",
    "    val_idx += class_indices[:split]\n",
    "    train_idx += class_indices[split:]\n",
    "\n",
    "# Defining the subsets for training and validation\n",
    "original_datasets = {\n",
    "    'train': Subset(dataset, train_idx),\n",
    "    'val': Subset(dataset, val_idx)\n",
    "}\n",
    "print('datasets have been created')\n",
    "\n",
    "original_dataloaders = {x: DataLoader(dataset=original_datasets[x], batch_size=batch_size, num_workers=2,\n",
    "                                      shuffle=True if x == 'train' else False, drop_last=True)\n",
    "                        for x in ['train', 'val']}\n",
    "print('dataloaders have been created')\n",
    "\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f'there are {num_classes} classes, and class names are {class_names}')\n",
    "\n",
    "class_counts_dict = {x: len(original_datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Dataset sizes: {class_counts_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f942236e3ab5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counting Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254fb4ff3ed2113",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for _, label in original_dataloaders[phase]:\n",
    "        class_counts.update(label.tolist())\n",
    "\n",
    "# show details\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count} instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e13b60b46a67c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Defining Augmentation Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c356a006813a5b0e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T14:53:16.044605400Z",
     "start_time": "2024-04-11T14:53:16.031605700Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomAugmentedDataset(Dataset):\n",
    "    def __init__(self, class_names, num_samples_per_class, root_dir=None, dataset=None, transform=None,\n",
    "                 num_magnitude_bins=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            num_samples_per_class (int): Desired number of samples per class after augmentation.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        if dataset:\n",
    "            self.dataset = dataset\n",
    "            self.situation = 'dataset'\n",
    "        else:\n",
    "            self.dataset = ImageFolder(root=root_dir)\n",
    "            self.situation = 'root_dir'\n",
    "        self.classes = class_names\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.transform = transform\n",
    "        self.augment_transform = transforms.Compose([\n",
    "            autoaugment.TrivialAugmentWide(num_magnitude_bins=num_magnitude_bins),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224))\n",
    "        ])\n",
    "        self.class_samples = self._balance_classes()\n",
    "\n",
    "    def _balance_classes(self):\n",
    "        from collections import defaultdict\n",
    "        class_indices = defaultdict(list)\n",
    "        if self.situation == 'root_dir':\n",
    "            for idx, (_, class_id) in enumerate(self.dataset.samples):\n",
    "                class_indices[class_id].append(idx)\n",
    "        else:\n",
    "            for idx, (_, class_id) in enumerate(self.dataset.samples):\n",
    "                class_indices[class_id].append(idx)\n",
    "\n",
    "        # Reduce or oversample class indices to match num_samples_per_class\n",
    "        balanced_indices = []\n",
    "        for indices in class_indices.values():\n",
    "            if len(indices) >= self.num_samples_per_class:\n",
    "                balanced_indices.extend(indices[:self.num_samples_per_class])\n",
    "            else:\n",
    "                # Oversample if there are fewer samples than desired\n",
    "                oversampled_indices = indices * (self.num_samples_per_class // len(indices)) + indices[\n",
    "                                                                                               :self.num_samples_per_class % len(\n",
    "                                                                                                   indices)]\n",
    "                balanced_indices.extend(oversampled_indices)\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[self.class_samples[idx]]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = self.augment_transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def classes(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b16905939cec57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creating Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b20c11b136a70725",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T14:53:18.114142300Z",
     "start_time": "2024-04-11T14:53:18.061143600Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'samples'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 12\u001B[0m\n\u001B[0;32m      6\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      7\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)),\n\u001B[0;32m      8\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor()\n\u001B[0;32m      9\u001B[0m ])\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Creating datasets\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m datasets \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     13\u001B[0m     x: CustomAugmentedDataset(class_names\u001B[38;5;241m=\u001B[39mclass_names, dataset\u001B[38;5;241m=\u001B[39moriginal_datasets[x], \n\u001B[0;32m     14\u001B[0m                               num_magnitude_bins\u001B[38;5;241m=\u001B[39mnum_magnitude_bins,\n\u001B[0;32m     15\u001B[0m                               num_samples_per_class\u001B[38;5;241m=\u001B[39mnum_samples_per_class \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m300\u001B[39m)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     17\u001B[0m }\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDatasets created.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Creating dataloaders\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[16], line 13\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      6\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      7\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)),\n\u001B[0;32m      8\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor()\n\u001B[0;32m      9\u001B[0m ])\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Creating datasets\u001B[39;00m\n\u001B[0;32m     12\u001B[0m datasets \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m---> 13\u001B[0m     x: \u001B[43mCustomAugmentedDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclass_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moriginal_datasets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mnum_magnitude_bins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_magnitude_bins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mnum_samples_per_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_samples_per_class\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     17\u001B[0m }\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDatasets created.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Creating dataloaders\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[15], line 24\u001B[0m, in \u001B[0;36mCustomAugmentedDataset.__init__\u001B[1;34m(self, class_names, num_samples_per_class, root_dir, dataset, transform, num_magnitude_bins)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;241m=\u001B[39m transform\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maugment_transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m     20\u001B[0m     autoaugment\u001B[38;5;241m.\u001B[39mTrivialAugmentWide(num_magnitude_bins\u001B[38;5;241m=\u001B[39mnum_magnitude_bins),\n\u001B[0;32m     21\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m     22\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m))\n\u001B[0;32m     23\u001B[0m ])\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_balance_classes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 33\u001B[0m, in \u001B[0;36mCustomAugmentedDataset._balance_classes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     31\u001B[0m         class_indices[class_id]\u001B[38;5;241m.\u001B[39mappend(idx)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 33\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, (_, class_id) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msamples\u001B[49m):\n\u001B[0;32m     34\u001B[0m         class_indices[class_id]\u001B[38;5;241m.\u001B[39mappend(idx)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Reduce or oversample class indices to match num_samples_per_class\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Subset' object has no attribute 'samples'"
     ]
    }
   ],
   "source": [
    "# Define Parameters\n",
    "num_magnitude_bins = 100\n",
    "num_samples_per_class = 3000\n",
    "\n",
    "# Define any additional transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Creating datasets\n",
    "datasets = {\n",
    "    x: CustomAugmentedDataset(class_names=class_names, dataset=original_datasets[x], \n",
    "                              num_magnitude_bins=num_magnitude_bins,\n",
    "                              num_samples_per_class=num_samples_per_class if x == 'train' else 300)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "print('Datasets created.')\n",
    "\n",
    "# Creating dataloaders\n",
    "batch_size = 32\n",
    "dataloaders = {\n",
    "    x: DataLoader(dataset=datasets[x], batch_size=batch_size, num_workers=2, shuffle=True if x == 'train' else False,\n",
    "                  drop_last=True)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "print('Dataloaders created.')\n",
    "print('-' * 50)\n",
    "\n",
    "# Show Classes\n",
    "class_names = datasets['train'].classes\n",
    "print(f'there are {len(class_names)} classes, and class names are {class_names}')\n",
    "print('-' * 50)\n",
    "\n",
    "# Show datasets length \n",
    "class_counts_dict = {x: len(datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Dataset sizes: {class_counts_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m datasets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(data)\n",
      "Cell \u001B[1;32mIn[3], line 57\u001B[0m, in \u001B[0;36mCustomAugmentedDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     55\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img, label\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:140\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    138\u001B[0m     _log_api_usage_once(to_tensor)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (F_pil\u001B[38;5;241m.\u001B[39m_is_pil_image(pic) \u001B[38;5;129;01mor\u001B[39;00m _is_numpy(pic)):\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be PIL Image or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy(pic) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_numpy_image(pic):\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be 2/3 dimensional. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "for data in datasets['train']:\n",
    "    print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T14:02:50.211084500Z",
     "start_time": "2024-04-11T14:02:50.152084600Z"
    }
   },
   "id": "a19638ac13ff5774"
  },
  {
   "cell_type": "markdown",
   "id": "2907536d36769fb6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counting Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0f2144b8d4dda",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-11T13:48:37.616617700Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for _, label in dataloaders[phase]:\n",
    "        class_counts.update(label.tolist())\n",
    "\n",
    "# show details\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count} instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4362cb281e0985",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Showing augmented data sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6fd339bc72cd98f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T14:49:01.998789700Z",
     "start_time": "2024-04-11T14:49:01.939133500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  2706\n",
      "idx type:  <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdiar\\AppData\\Local\\Temp\\ipykernel_7336\\79890326.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (image, label) = datasets['train'][int(idx)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124midx: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mint\u001B[39m(idx\u001B[38;5;241m.\u001B[39mitem()))\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124midx type: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m(idx))\n\u001B[1;32m----> 7\u001B[0m (image, label) \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage type: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m(image))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel: \u001B[39m\u001B[38;5;124m'\u001B[39m, label)\n",
      "Cell \u001B[1;32mIn[3], line 57\u001B[0m, in \u001B[0;36mCustomAugmentedDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     55\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img, label\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:140\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    138\u001B[0m     _log_api_usage_once(to_tensor)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (F_pil\u001B[38;5;241m.\u001B[39m_is_pil_image(pic) \u001B[38;5;129;01mor\u001B[39;00m _is_numpy(pic)):\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be PIL Image or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy(pic) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_numpy_image(pic):\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be 2/3 dimensional. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = np.random.randint(0, 6000, size=1)\n",
    "\n",
    "print('idx: ', int(idx.item()))\n",
    "print('idx type: ', type(idx))\n",
    "(image, label) = datasets['train'][int(idx)]\n",
    "print('image type: ', type(image))\n",
    "print('label: ', label)\n",
    "print('class name label: ', class_names[label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146e0b7ca1cfad8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load ViT pretrained on DINOv2 with registers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8b394f43e584f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DINOv2\n",
    "dinov2_vits14_21M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "dinov2_vitb14_86M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# DINOv2 with registers\n",
    "dinov2_vits14_reg_21M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "dinov2_vitb14_reg_86M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "# dinov2_vitl14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "# dinov2_vitg14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67796172e50e4566",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fine-Tune model classifier and trainable parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcde227a150ca83",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = dinov2_vits14_reg_21M\n",
    "\n",
    "# Define classifier for Binary Classification task\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(384, len(class_names))\n",
    ")\n",
    "print(dinov2_vits14_reg_21M)\n",
    "\n",
    "# Set about 30% of parameters trainable \n",
    "model_params = 0\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    param.requires_grad = False\n",
    "    model_params += 1\n",
    "    if idx == 125:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8eff45af7cf71e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining Train function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51a45e8fe06df4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "\n",
    "# train function \n",
    "def train_model(model, criterion, optimizer, dataloaders, datasets, epoch_num=25):\n",
    "    acc_list = EasyDict({'train': [], 'val': []})\n",
    "    loss_list = EasyDict({'train': [], 'val': []})\n",
    "\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            s0 = datetime.now()\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over batches and data for training and validation\n",
    "            for idx, batch in enumerate(dataloaders[phase], 0):\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Transfer data and labels to CUDA if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    assert outputs.shape[1] == 15, \"Output size does not match number of classes\"\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    assert labels.min() >= 0 and labels.max() < 15, \"Labels are out of range\"\n",
    "\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                    # Back Propagation and updating weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "            # Calculating Accuracy and Loss per phase\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_accuracy = running_corrects / len(datasets[phase])\n",
    "\n",
    "            # Show epoch details\n",
    "            delta = datetime.now() - s0\n",
    "            print(f'{phase.capitalize()} Accuracy: {epoch_accuracy:.4f} | Loss: {epoch_loss:.4f} | time: {delta}')\n",
    "\n",
    "            # Copy the model weights if its better\n",
    "            if phase == 'val' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('Best model weights updated!')\n",
    "\n",
    "            # Save Loss and accuracy\n",
    "            acc_list[phase].append(epoch_accuracy)\n",
    "            loss_list[phase].append(epoch_loss)\n",
    "        print('-' * 50)\n",
    "\n",
    "    print(f'Best Accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "    # Loading best model weights \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_list, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92ba5a34d052b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train ViT-s DINOv2 with registers \n",
    "---------------\n",
    "## Hyperparameters:\n",
    "### optimizer: Adam\n",
    "### criterion: CrossEntropy\n",
    "### Learning Rate: 0.001\n",
    "### batch size: 32\n",
    "### epoch: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f402be2ffac6a",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "criterion = CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}', end='\\n\\n')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print('-' * 50)\n",
    "\n",
    "# train model\n",
    "model, acc_lists, loss_lists = train_model(model, criterion, optimizer, dataloaders, datasets, epoch_num=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f987c83ba60f81",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plot Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1662086cdffb60c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_lists.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d57ed078c0e3c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([a for a in loss_lists.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists.val], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611bf71796f956ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists.train], label='train acc')\n",
    "plt.plot([a.cpu() for a in acc_lists.val], label='val acc')\n",
    "plt.plot([a for a in loss_lists.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists.val], label='val loss')\n",
    "plt.title('result')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827c1fc21b70f2d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cadb4080c327e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'models/model_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63985af179cb400b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualize model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640df5d24d8c574e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    model.eval()\n",
    "    nrows, ncols = 4, 4\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                img = inputs.cpu().data[j]\n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                # img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                axes[i][j].axis('off')\n",
    "                axes[i][j].set_title(\n",
    "                    f'predictions: {class_names[predictions[j]]}, label: {class_names[labels[j]]}'\n",
    "                )\n",
    "                axes[i][j].imshow(img)\n",
    "                if j == ncols - 1:\n",
    "                    break\n",
    "            if i == nrows - 1:\n",
    "                break\n",
    "    plt.savefig('vis.jpg')\n",
    "\n",
    "\n",
    "model = torch.load('models/model_2.pth')\n",
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe14eaef26dbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plot Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4e930e8218959",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_cm(model):\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
    "            y_pred.extend(outputs)\n",
    "\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cm / np.sum(cm, axis=1)[:, None],\n",
    "        index=[i for i in class_names],\n",
    "        columns=[i for i in class_names]\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(df_cm, annot=True, cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_cm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdfa0c0831d0cb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fine-Tune model2 classifier and trainable parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fcd5629682bb6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model2 = dinov2_vits14_reg_21M\n",
    "\n",
    "# Define classifier for Binary Classification task\n",
    "model2.head = nn.Sequential(\n",
    "    nn.Linear(384, 2)\n",
    ")\n",
    "\n",
    "# Set about 30% of parameters trainable \n",
    "model_params = 0\n",
    "for idx, param in enumerate(model2.parameters()):\n",
    "    param.requires_grad = False\n",
    "    model_params += 1\n",
    "    if idx == 125:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525edccf787b17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train ViT-s DINOv2 with registers \n",
    "---------------\n",
    "## Hyperparameters:\n",
    "### optimizer: Adam\n",
    "### criterion: CrossEntropy\n",
    "### Learning Rate: 0.0003\n",
    "### batch size: 32\n",
    "### epoch: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a14a9f3c363f2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining Hyperparameters \n",
    "criterion = CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}', end='\\n\\n')\n",
    "optimizer = Adam(model2.parameters(), lr=0.0003)\n",
    "model2 = model2.to(device)\n",
    "print(model2)\n",
    "print('-' * 50)\n",
    "\n",
    "# train model\n",
    "model2, acc_lists2, loss_lists2 = train_model(model2, criterion, optimizer, dataloaders, datasets, epoch_num=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bc46a7f508b8c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa56938df7d88df",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists2.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_lists2.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953edc34415d7aa3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([a for a in loss_lists2.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists2.val], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23aba7bc0c815e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists2.train], label='train acc')\n",
    "plt.plot([a.cpu() for a in acc_lists2.val], label='val acc')\n",
    "plt.plot([a for a in loss_lists2.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists2.val], label='val loss')\n",
    "plt.title('result')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902be7f60a3c3c8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualize model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5bf061e5894d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    model.eval()\n",
    "    nrows, ncols = 4, 4\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                img = inputs.cpu().data[j]\n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                # img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                axes[i][j].axis('off')\n",
    "                axes[i][j].set_title(\n",
    "                    f'predictions: {class_names[predictions[j]]}, label: {class_names[labels[j]]}'\n",
    "                )\n",
    "                axes[i][j].imshow(img)\n",
    "                if j == ncols - 1:\n",
    "                    break\n",
    "            if i == nrows - 1:\n",
    "                break\n",
    "    plt.savefig('vis.jpg')\n",
    "\n",
    "\n",
    "visualize_model(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc545b1b94b38a73",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7dc97ee490f61",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'models/model_3.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
