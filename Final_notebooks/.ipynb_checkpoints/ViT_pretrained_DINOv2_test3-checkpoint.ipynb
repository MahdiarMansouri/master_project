{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ViT Small Pretrained on DINOv2 with registers \n",
    "## Data: Myxococcaceae vs non-Myxococcaceae\n",
    "## Augmentation: TrivialAugmentWide to 60000 samples\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1410a8d31948e1e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### import requirements"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c10a7389c7dee4cc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:57:48.412827300Z",
     "start_time": "2024-04-10T14:57:46.665668900Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms, autoaugment\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da703ffb580a457e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Original Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94a6895498d51d47"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your device is cuda\n",
      "datasets have been created\n",
      "dataloaders have been created\n",
      "there are 2 classes, and class names are ['Myxococcaceae', 'non-Myxococcaceae']\n",
      "Dataset sizes: {'train': 4558, 'val': 1139}\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}')\n",
    "\n",
    "# Defining data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop((224, 224))\n",
    "])\n",
    "\n",
    "data_path = 'D:\\Master Project\\model\\model-1\\myxo-vs-nonmyxo-V2-9p'\n",
    "dataset = ImageFolder(root=data_path, transform=data_transforms)\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "# Create indices for splitting\n",
    "targets = torch.tensor(dataset.targets)\n",
    "class_counts = targets.unique(return_counts=True)[1]\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Calculating indices for each class to maintain the ratio\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_idx, val_idx = [], []\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    class_indices = [i for i in range(len(targets)) if targets[i] == class_index]\n",
    "    split = int(len(class_indices) * validation_split)\n",
    "    class_indices = torch.tensor(class_indices)[torch.randperm(len(class_indices))].tolist()\n",
    "    \n",
    "    val_idx += class_indices[:split]\n",
    "    train_idx += class_indices[split:]\n",
    "\n",
    "# Defining the subsets for training and validation\n",
    "original_datasets = {\n",
    "    'train': Subset(dataset, train_idx),\n",
    "    'val': Subset(dataset, val_idx)\n",
    "}\n",
    "print('datasets have been created')\n",
    "\n",
    "original_dataloaders = {x: DataLoader(dataset=original_datasets[x], batch_size=batch_size, num_workers=2,\n",
    "                                      shuffle=True if x == 'train' else False, drop_last=True)\n",
    "                        for x in ['train', 'val']}\n",
    "print('dataloaders have been created')\n",
    "\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f'there are {num_classes} classes, and class names are {class_names}')\n",
    "\n",
    "class_counts_dict = {x: len(original_datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Dataset sizes: {class_counts_dict}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:57:48.529437300Z",
     "start_time": "2024-04-10T14:57:48.415817600Z"
    }
   },
   "id": "fb23dc65e3ffa848"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting Classes "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "548f942236e3ab5a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 4847 instances\n",
      "Class 1: 817 instances\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for _, label in original_dataloaders[phase]:\n",
    "        class_counts.update(label.tolist())\n",
    "\n",
    "# show details\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count} instances')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:58:03.266796100Z",
     "start_time": "2024-04-10T14:57:49.602086900Z"
    }
   },
   "id": "5254fb4ff3ed2113"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining Augmentation Class "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b8e13b60b46a67c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CustomAugmentedDataset(Dataset):\n",
    "    def __init__(self, class_names, num_samples_per_class, root_dir=None, dataset=None, transform=None,\n",
    "                 num_magnitude_bins=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            num_samples_per_class (int): Desired number of samples per class after augmentation.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        if dataset:\n",
    "            self.dataset = dataset\n",
    "            self.situation = 'dataset'\n",
    "        else:\n",
    "            self.dataset = ImageFolder(root=root_dir)\n",
    "            self.situation = 'root_dir'\n",
    "        self.classes = class_names\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.transform = transform\n",
    "        self.augment_transform = transforms.Compose([\n",
    "            autoaugment.TrivialAugmentWide(num_magnitude_bins=num_magnitude_bins),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.class_samples = self._balance_classes()\n",
    "\n",
    "    def _balance_classes(self):\n",
    "        from collections import defaultdict\n",
    "        class_indices = defaultdict(list)\n",
    "        if self.situation == 'root_dir':\n",
    "            for idx, (_, class_id) in enumerate(self.dataset.samples):\n",
    "                class_indices[class_id].append(idx)\n",
    "        else:\n",
    "            for idx, (_, class_id) in enumerate(self.dataset):\n",
    "                class_indices[class_id].append(idx)\n",
    "\n",
    "        # Reduce or oversample class indices to match num_samples_per_class\n",
    "        balanced_indices = []\n",
    "        for indices in class_indices.values():\n",
    "            if len(indices) >= self.num_samples_per_class:\n",
    "                balanced_indices.extend(indices[:self.num_samples_per_class])\n",
    "            else:\n",
    "                # Oversample if there are fewer samples than desired\n",
    "                oversampled_indices = indices * (self.num_samples_per_class // len(indices)) + indices[\n",
    "                                                                                               :self.num_samples_per_class % len(\n",
    "                                                                                                   indices)]\n",
    "                balanced_indices.extend(oversampled_indices)\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[self.class_samples[idx]]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = self.augment_transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def classes(self):\n",
    "        return self.classes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:58:05.700234700Z",
     "start_time": "2024-04-10T14:58:05.652393900Z"
    }
   },
   "id": "c356a006813a5b0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating Augmented Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4b16905939cec57"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdiar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created.\n",
      "Dataloaders created.\n",
      "--------------------------------------------------\n",
      "there are 2 classes, and class names are ['Myxococcaceae', 'non-Myxococcaceae']\n",
      "--------------------------------------------------\n",
      "Dataset sizes: {'train': 60000, 'val': 6000}\n"
     ]
    }
   ],
   "source": [
    "# Define Parameters\n",
    "num_magnitude_bins = 100\n",
    "num_samples_per_class = 30000\n",
    "\n",
    "# Define any additional transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Creating datasets\n",
    "datasets = {\n",
    "    x: CustomAugmentedDataset(class_names=class_names, dataset=original_datasets[x], transform=transform,\n",
    "                              num_magnitude_bins=num_magnitude_bins,\n",
    "                              num_samples_per_class=num_samples_per_class if x == 'train' else 3000)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "print('Datasets created.')\n",
    "\n",
    "# Creating dataloaders\n",
    "batch_size = 32\n",
    "dataloaders = {\n",
    "    x: DataLoader(dataset=datasets[x], batch_size=batch_size, num_workers=2, shuffle=True if x == 'train' else False,\n",
    "                  drop_last=True)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "print('Dataloaders created.')\n",
    "print('-' * 50)\n",
    "\n",
    "# Show Classes\n",
    "class_names = datasets['train'].classes\n",
    "print(f'there are {len(class_names)} classes, and class names are {class_names}')\n",
    "print('-' * 50)\n",
    "\n",
    "# Show datasets length \n",
    "class_counts_dict = {x: len(datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Dataset sizes: {class_counts_dict}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:58:13.859099600Z",
     "start_time": "2024-04-10T14:58:07.732217900Z"
    }
   },
   "id": "b20c11b136a70725"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting Classes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2907536d36769fb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter()\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for _, label in dataloaders[phase]:\n",
    "        class_counts.update(label.tolist())\n",
    "\n",
    "# show details\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count} instances')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:58:37.281923900Z"
    }
   },
   "id": "21a0f2144b8d4dda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Showing augmented data sample "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb4362cb281e0985"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdiar\\AppData\\Local\\Temp\\ipykernel_14800\\73173309.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('idx: ', int(idx))\n",
      "C:\\Users\\Mahdiar\\AppData\\Local\\Temp\\ipykernel_14800\\73173309.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  image, label = datasets['train'][int(idx)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  21637\n",
      "idx type:  <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124midx: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mint\u001B[39m(idx))\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124midx type: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m(idx))\n\u001B[1;32m----> 7\u001B[0m image, label \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage type: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m(image))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel: \u001B[39m\u001B[38;5;124m'\u001B[39m, label)\n",
      "Cell \u001B[1;32mIn[4], line 55\u001B[0m, in \u001B[0;36mCustomAugmentedDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     53\u001B[0m img, label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_samples[idx]]\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 55\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maugment_transform(img)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:140\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    138\u001B[0m     _log_api_usage_once(to_tensor)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (F_pil\u001B[38;5;241m.\u001B[39m_is_pil_image(pic) \u001B[38;5;129;01mor\u001B[39;00m _is_numpy(pic)):\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be PIL Image or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy(pic) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_numpy_image(pic):\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be 2/3 dimensional. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = np.random.randint(0, 60000, size=1)\n",
    "\n",
    "print('idx: ', int(idx))\n",
    "print('idx type: ', type(idx))\n",
    "image, label = datasets['train'][int(idx)]\n",
    "print('image type: ', type(image))\n",
    "print('label: ', label)\n",
    "print('class name label: ', class_names[label])\n",
    "\n",
    "# Convert torch tensor for plotting\n",
    "image = image.permute(1, 2, 0)\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:58:23.974128400Z",
     "start_time": "2024-04-10T14:58:23.650210500Z"
    }
   },
   "id": "f6fd339bc72cd98f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load ViT pretrained on DINOv2 with registers model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4146e0b7ca1cfad8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DINOv2\n",
    "dinov2_vits14_21M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "dinov2_vitb14_86M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# DINOv2 with registers\n",
    "dinov2_vits14_reg_21M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "dinov2_vitb14_reg_86M = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "# dinov2_vitl14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "# dinov2_vitg14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:34:42.476236Z",
     "start_time": "2024-04-10T14:34:42.475243800Z"
    }
   },
   "id": "d7e8b394f43e584f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-Tune model classifier and trainable parameters "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67796172e50e4566"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = dinov2_vits14_reg_21M\n",
    "\n",
    "# Define classifier for Binary Classification task\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(384, len(class_names))\n",
    ")\n",
    "print(dinov2_vits14_reg_21M)\n",
    "\n",
    "# Set about 30% of parameters trainable \n",
    "model_params = 0\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    param.requires_grad = False\n",
    "    model_params += 1\n",
    "    if idx == 125:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:34:42.477233Z",
     "start_time": "2024-04-10T14:34:42.476236Z"
    }
   },
   "id": "abcde227a150ca83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Train function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8eff45af7cf71e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "\n",
    "# train function \n",
    "def train_model(model, criterion, optimizer, dataloaders, datasets, epoch_num=25):\n",
    "    acc_list = EasyDict({'train': [], 'val': []})\n",
    "    loss_list = EasyDict({'train': [], 'val': []})\n",
    "\n",
    "    # Copy the best model weights for loading at the End\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Iterating over epochs\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print(f'Epoch {epoch}/{epoch_num}:')\n",
    "\n",
    "        # Each epoch has two phase Train and Validation\n",
    "        for phase in ['train', 'val']:\n",
    "            s0 = datetime.now()\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # For calculating Loss and Accuracy at the end of epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterating over batches and data for training and validation\n",
    "            for idx, batch in enumerate(dataloaders[phase], 0):\n",
    "                inputs, labels = batch\n",
    "\n",
    "                # Transfer data and labels to CUDA if is available\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward Pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    assert outputs.shape[1] == 15, \"Output size does not match number of classes\"\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    assert labels.min() >= 0 and labels.max() < 15, \"Labels are out of range\"\n",
    "\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                    # Back Propagation and updating weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "            # Calculating Accuracy and Loss per phase\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_accuracy = running_corrects / len(datasets[phase])\n",
    "\n",
    "            # Show epoch details\n",
    "            delta = datetime.now() - s0\n",
    "            print(f'{phase.capitalize()} Accuracy: {epoch_accuracy:.4f} | Loss: {epoch_loss:.4f} | time: {delta}')\n",
    "\n",
    "            # Copy the model weights if its better\n",
    "            if phase == 'val' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('Best model weights updated!')\n",
    "\n",
    "            # Save Loss and accuracy\n",
    "            acc_list[phase].append(epoch_accuracy)\n",
    "            loss_list[phase].append(epoch_loss)\n",
    "        print('-' * 50)\n",
    "\n",
    "    print(f'Best Accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "    # Loading best model weights \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_list, loss_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T14:34:42.490006600Z",
     "start_time": "2024-04-10T14:34:42.477233Z"
    }
   },
   "id": "ca51a45e8fe06df4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train ViT-s DINOv2 with registers \n",
    "---------------\n",
    "## Hyperparameters:\n",
    "### optimizer: Adam\n",
    "### criterion: CrossEntropy\n",
    "### Learning Rate: 0.001\n",
    "### batch size: 32\n",
    "### epoch: 50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f92ba5a34d052b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "criterion = CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}', end='\\n\\n')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print('-' * 50)\n",
    "\n",
    "# train model\n",
    "model, acc_lists, loss_lists = train_model(model, criterion, optimizer, dataloaders, datasets, epoch_num=40)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.478229600Z"
    }
   },
   "id": "725f402be2ffac6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Results "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f987c83ba60f81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_lists.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.479226300Z"
    }
   },
   "id": "a1662086cdffb60c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a for a in loss_lists.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists.val], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.480222900Z"
    }
   },
   "id": "90d57ed078c0e3c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists.train], label='train acc')\n",
    "plt.plot([a.cpu() for a in acc_lists.val], label='val acc')\n",
    "plt.plot([a for a in loss_lists.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists.val], label='val loss')\n",
    "plt.title('result')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.481219700Z"
    }
   },
   "id": "611bf71796f956ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save best model weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e827c1fc21b70f2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model, 'models/model_2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.483213Z"
    }
   },
   "id": "95cadb4080c327e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize model predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63985af179cb400b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    model.eval()\n",
    "    nrows, ncols = 4, 4\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                img = inputs.cpu().data[j]\n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                # img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                axes[i][j].axis('off')\n",
    "                axes[i][j].set_title(\n",
    "                    f'predictions: {class_names[predictions[j]]}, label: {class_names[labels[j]]}'\n",
    "                )\n",
    "                axes[i][j].imshow(img)\n",
    "                if j == ncols - 1:\n",
    "                    break\n",
    "            if i == nrows - 1:\n",
    "                break\n",
    "    plt.savefig('vis.jpg')\n",
    "\n",
    "\n",
    "model = torch.load('models/model_2.pth')\n",
    "visualize_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.484209700Z"
    }
   },
   "id": "640df5d24d8c574e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8fe14eaef26dbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_cm(model):\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
    "            y_pred.extend(outputs)\n",
    "\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cm / np.sum(cm, axis=1)[:, None],\n",
    "        index=[i for i in class_names],\n",
    "        columns=[i for i in class_names]\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(df_cm, annot=True, cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_cm(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.485205300Z"
    }
   },
   "id": "65e4e930e8218959"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-Tune model2 classifier and trainable parameters "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68cdfa0c0831d0cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define model\n",
    "model2 = dinov2_vits14_reg_21M\n",
    "\n",
    "# Define classifier for Binary Classification task\n",
    "model2.head = nn.Sequential(\n",
    "    nn.Linear(384, 2)\n",
    ")\n",
    "\n",
    "# Set about 30% of parameters trainable \n",
    "model_params = 0\n",
    "for idx, param in enumerate(model2.parameters()):\n",
    "    param.requires_grad = False\n",
    "    model_params += 1\n",
    "    if idx == 125:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.486024400Z"
    }
   },
   "id": "4e4fcd5629682bb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train ViT-s DINOv2 with registers \n",
    "---------------\n",
    "## Hyperparameters:\n",
    "### optimizer: Adam\n",
    "### criterion: CrossEntropy\n",
    "### Learning Rate: 0.0003\n",
    "### batch size: 32\n",
    "### epoch: 50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e525edccf787b17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining Hyperparameters \n",
    "criterion = CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'your device is {device}', end='\\n\\n')\n",
    "optimizer = Adam(model2.parameters(), lr=0.0003)\n",
    "model2 = model2.to(device)\n",
    "print(model2)\n",
    "print('-' * 50)\n",
    "\n",
    "# train model\n",
    "model2, acc_lists2, loss_lists2 = train_model(model2, criterion, optimizer, dataloaders, datasets, epoch_num=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.488013900Z"
    }
   },
   "id": "4f5a14a9f3c363f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d13bc46a7f508b8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists2.train], label='train')\n",
    "plt.plot([a.cpu() for a in acc_lists2.val], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.488013900Z"
    }
   },
   "id": "7fa56938df7d88df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a for a in loss_lists2.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists2.val], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Percent')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.489010300Z"
    }
   },
   "id": "953edc34415d7aa3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot([a.cpu() for a in acc_lists2.train], label='train acc')\n",
    "plt.plot([a.cpu() for a in acc_lists2.val], label='val acc')\n",
    "plt.plot([a for a in loss_lists2.train], label='train loss')\n",
    "plt.plot([a for a in loss_lists2.val], label='val loss')\n",
    "plt.title('result')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Percent')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.490006600Z"
    }
   },
   "id": "f23aba7bc0c815e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize model predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "902be7f60a3c3c8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    model.eval()\n",
    "    nrows, ncols = 4, 4\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                img = inputs.cpu().data[j]\n",
    "                img = img.numpy().transpose((1, 2, 0))\n",
    "                # img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                axes[i][j].axis('off')\n",
    "                axes[i][j].set_title(\n",
    "                    f'predictions: {class_names[predictions[j]]}, label: {class_names[labels[j]]}'\n",
    "                )\n",
    "                axes[i][j].imshow(img)\n",
    "                if j == ncols - 1:\n",
    "                    break\n",
    "            if i == nrows - 1:\n",
    "                break\n",
    "    plt.savefig('vis.jpg')\n",
    "\n",
    "\n",
    "visualize_model(model2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.491002900Z"
    }
   },
   "id": "a5d5bf061e5894d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save best model weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc545b1b94b38a73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model, 'models/model_3.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-10T14:34:42.524889600Z"
    }
   },
   "id": "2ae7dc97ee490f61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
